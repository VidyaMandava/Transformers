{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Using Numpy**"
      ],
      "metadata": {
        "id": "Rq_gklrCZSS4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0kDVY2bZIGB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Seed for reproducibility\n",
        "# np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input matrix (3 words, 4-dimensional embeddings)\n",
        "X = np.array([[0.2, 0.8, 0.5, 0.1],  # Word 1\n",
        "              [0.9, 0.4, 0.7, 0.3],  # Word 2\n",
        "              [0.5, 0.1, 0.2, 0.9]]) # Word 3"
      ],
      "metadata": {
        "id": "h31fZRHBZZhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define trainable weight matrices (random initialization)\n",
        "d_k = X.shape[1]  # Dimension of word embeddings (4)\n",
        "\n",
        "W_q = np.random.rand(d_k, d_k)  # Query weight matrix\n",
        "W_k = np.random.rand(d_k, d_k)  # Key weight matrix\n",
        "W_v = np.random.rand(d_k, d_k)  # Value weight matrix\n",
        "\n",
        "# Compute Q, K, V matrices\n",
        "Q = np.dot(X, W_q)\n",
        "K = np.dot(X, W_k)\n",
        "V = np.dot(X, W_v)\n",
        "\n",
        "print(\"Query Matrix:\\n\", Q)\n",
        "print(\"Key Matrix:\\n\", K)\n",
        "print(\"Value Matrix:\\n\", V)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psgNG4CjZmLW",
        "outputId": "3f044117-a868-4070-951d-7b51d4607d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Matrix:\n",
            " [[0.58352471 0.69020868 0.22134042 1.31596799]\n",
            " [1.07000686 1.47739322 0.75098463 1.61922134]\n",
            " [1.0722933  0.82367632 0.5395647  0.74499289]]\n",
            "Key Matrix:\n",
            " [[0.83760721 0.6137794  0.4806961  0.62550493]\n",
            " [1.01553254 1.09163663 0.82764348 0.81977231]\n",
            " [0.83769352 0.47516817 0.8319121  0.43856935]]\n",
            "Value Matrix:\n",
            " [[0.34359845 0.58175548 0.7888782  1.02046847]\n",
            " [0.3434527  1.43844635 1.26034733 1.6961633 ]\n",
            " [0.3202968  1.17951542 0.83865699 1.09813922]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute raw attention scores (QK^T)\n",
        "attention_scores = np.dot(Q, K.T)\n",
        "\n",
        "# Scale by sqrt(d_k)\n",
        "attention_scores /= np.sqrt(d_k)\n",
        "\n",
        "print(\"Raw Attention Scores:\\n\", attention_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOyiXCryaENY",
        "outputId": "dd738320-97be-4aa2-e205-22b4d19bf934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Attention Scores:\n",
            " [[0.92097116 1.30401524 0.78902953]\n",
            " [1.58843265 2.32417184 1.46662105]\n",
            " [1.06453977 1.52269786 1.03261968]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply softmax to normalize scores\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
        "\n",
        "attention_weights = softmax(attention_scores)\n",
        "\n",
        "print(\"Attention Weights:\\n\", attention_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11oVZu2daZ-N",
        "outputId": "492ca7f2-6480-4aec-dd40-18dff3f7f85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " [[0.29912045 0.43873273 0.26214682]\n",
            " [0.25174083 0.52538919 0.22286997]\n",
            " [0.28171054 0.44542914 0.27286032]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiply weights by Value (V)\n",
        "attention_output = np.dot(attention_weights, V)\n",
        "\n",
        "print(\"Final Attention Output:\\n\", attention_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OJps1t2an2D",
        "outputId": "6e388fa7-1a59-4e3f-acd9-1434e832036f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Attention Output:\n",
            " [[0.33742605 1.11431467 1.00877649 1.33727905]\n",
            " [0.33832864 1.16507435 1.04767718 1.39278171]\n",
            " [0.33717543 1.12645553 1.01246695 1.3426359 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "**Using PyTorch**"
      ],
      "metadata": {
        "id": "NwmFKcjZa0GX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Define input (batch_size=1, seq_len=3, embedding_dim=4)\n",
        "X_torch = torch.tensor(X, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "kFRoMN21atcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgm1qbAqbTmj",
        "outputId": "83e68878-3223-4388-c0bb-3e31a9770077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2000, 0.8000, 0.5000, 0.1000],\n",
              "        [0.9000, 0.4000, 0.7000, 0.3000],\n",
              "        [0.5000, 0.1000, 0.2000, 0.9000]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define weight matrices\n",
        "W_q = torch.rand((d_k, d_k), requires_grad=True)\n",
        "W_k = torch.rand((d_k, d_k), requires_grad=True)\n",
        "W_v = torch.rand((d_k, d_k), requires_grad=True)\n",
        "\n",
        "# Compute Q, K, V\n",
        "Q = X_torch @ W_q\n",
        "K = X_torch @ W_k\n",
        "V = X_torch @ W_v\n",
        "\n",
        "print(\"Query Matrix (PyTorch):\\n\", Q)\n",
        "print(\"Key Matrix (PyTorch):\\n\", K)\n",
        "print(\"Value Matrix (PyTorch):\\n\", V)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOZ7i5jObQbD",
        "outputId": "6c2bf633-361c-4b84-9c7d-e52384627ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Matrix (PyTorch):\n",
            " tensor([[1.0462, 0.9843, 0.9905, 0.4263],\n",
            "        [1.3142, 1.7460, 1.3134, 1.1299],\n",
            "        [0.9985, 1.5294, 0.5645, 0.8287]], grad_fn=<MmBackward0>)\n",
            "Key Matrix (PyTorch):\n",
            " tensor([[1.3015, 0.8496, 0.6376, 0.7813],\n",
            "        [1.7786, 1.1751, 0.9665, 0.6145],\n",
            "        [1.1877, 0.6539, 0.9527, 0.6404]], grad_fn=<MmBackward0>)\n",
            "Value Matrix (PyTorch):\n",
            " tensor([[0.4930, 0.6533, 1.1594, 1.0376],\n",
            "        [1.1325, 1.0038, 1.4088, 1.2255],\n",
            "        [0.9825, 0.6386, 0.5909, 0.7084]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute scaled attention scores\n",
        "attention_scores = (Q @ K.T) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "# Apply softmax\n",
        "attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "# Compute final output\n",
        "attention_output = attention_weights @ V\n",
        "\n",
        "print(\"Final Attention Output (PyTorch):\\n\", attention_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpSVbpO0bjzE",
        "outputId": "17517eee-db81-433a-fe41-45bb028a20d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Attention Output (PyTorch):\n",
            " tensor([[0.9193, 0.8123, 1.1254, 1.0381],\n",
            "        [0.9365, 0.8326, 1.1611, 1.0613],\n",
            "        [0.9142, 0.8164, 1.1419, 1.0482]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rvWO6UFhb0xl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}